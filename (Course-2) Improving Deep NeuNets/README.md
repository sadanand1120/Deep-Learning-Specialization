### Course 2
# Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization

## Course Certificate: [Verified by Coursera](https://www.coursera.org/account/accomplishments/certificate/6XS8HQ9B99BZ)

## Assignments
1. [Parameter Initialization techniques](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-2)%20Improving%20Deep%20NeuNets/Parameter%20Initialization%20techniques/Initialization.ipynb): Trying out different initialization techniques for the parameters such as Constant, Random, Xavier, and He initialization and seeing which one works the best.

2. [Gradient Checking](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-2)%20Improving%20Deep%20NeuNets/Gradient%20Checking/Gradient%20Checking%20v1.ipynb): Implementing the process of Gradient Checking, which beomes quite essential when we are explicitly coding for implementing the BackProp in our model (ie, building our model without any Deep Learning Framework).

3. [Regularization techniques](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-2)%20Improving%20Deep%20NeuNets/Regularization%20techniques/Regularization_v2a.ipynb): Implementing the standard L2 Regularization and Dropout Regularization to solve the problem of overfitting/high variance.

4. [Advanced Optimization Algorithms](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-2)%20Improving%20Deep%20NeuNets/Advanced%20Optimization%20Algorithms/Optimization_methods_v1b.ipynb): Implementing the standard Gradient Descent algorithm to minimize the cost function, and also the advanced algorithms like Momentum, RMSprop, and Adam.

5. [TensorFlow DL Framework](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-2)%20Improving%20Deep%20NeuNets/TensorFlow%20DL%20Framework/TensorFlow_Tutorial_v3b.ipynb): Basics of TensorFlow, a popular Deep Learning Framework, and how to use it effectively to build DeepNets.
