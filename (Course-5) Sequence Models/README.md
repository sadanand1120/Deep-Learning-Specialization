### Course 5
# Sequence Models

## Course Certificate: [Verified by Coursera](https://www.coursera.org/account/accomplishments/certificate/JMW7RA9FZ255)

## Assignments
1. [Standard Recurrent NN and LSTM](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Standard%20Recurrent%20NN%20and%20LSTM/Building_a_Recurrent_Neural_Network_Step_by_Step_v3b.ipynb): Building a basic RNN and implementing the standard RNN block. After that, implementing an LSTM cell in place of standard RNN block.

2. [Text Generation via Language Modeling](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Text%20Generation%20via%20Language%20Modeling/Dinosaurus_Island_Character_level_language_model_final_v3b.ipynb): Implementing RNN model for Text Generation so as to generate new Dinosaur names from existing names.

3. [Music Generation LSTM](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Music%20Generation%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v3a.ipynb): Generating new music via LSTM RNN.

4. [Analogy using Word-Embedding and Debiasing](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Analogy%20using%20Word-Embedding%20and%20Debiasing/Operations_on_word_vectors_v2a.ipynb): Using the embedded vector representation of words pre-trained using GloVe algorithm, and figuring out the Analogy logic between different pair of words. Also, implementing Debiasing to remove some bias in our embedding vectors.

5. [Emojifier using 2-Layer LSTM](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Emojifier%20using%202-Layer%20LSTM/Emojify_v2a.ipynb): Performing Sentiment Classification by predicting an emoji for every line of text, via a 2-Layer LSTM RNN.

6. [Neural Machine Translation with Attention Mechanism](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Neural%20Machine%20Translation%20with%20Attention%20Mechanism/Neural_machine_translation_with_attention_v4a.ipynb): Implementing the Attention Block in a 2-Layer LSTM Net to translate each given textual date to a format which a machine can understand.

7. [Trigger Word Detection via GRU DeepNet](https://github.com/sadanand1120/Deep-Learning-Specialization/blob/master/(Course-5)%20Sequence%20Models/Trigger%20Word%20Detection%20via%20GRU%20DeepNet/Trigger_word_detection_v1a.ipynb): Synthesising training data from given background and trigger word data, and then building a GRU Deep Network to detect the presence of trigger word in any given sound data.
